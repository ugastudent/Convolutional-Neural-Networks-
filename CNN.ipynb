{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executeConvolutionalNeuralNetwork\n",
      "__init__\n",
      "<class '__main__.ConvolutionalNeuralNetwork'>\n",
      "Accuracy : 0.8025\n",
      "ali\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import math\n",
    "import time\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################################################################\n",
    "\" The Convolutional Neural Network class \"\"\"\n",
    "\n",
    "class ConvolutionalNeuralNetwork(object):\n",
    "\n",
    "    #######################################################################################\n",
    "    \"\"\" Initialization of the network \"\"\"\n",
    "\n",
    "    def __init__(self, W1, b1, zca_white, mean_patch, patch_dim, pool_dim):\n",
    "        \n",
    "        print(\"__init__\")\n",
    "    \n",
    "        \"\"\" Store the weights, taking into account preprocessing done \"\"\"\n",
    "    \n",
    "        self.W = numpy.dot(W1, zca_white)\n",
    "        self.b = b1 - numpy.dot(self.W, mean_patch)\n",
    "        \n",
    "        \"\"\" Variables associated with the network \"\"\"\n",
    "        \n",
    "        self.patch_dim = patch_dim\n",
    "        self.pool_dim  = pool_dim\n",
    "\n",
    "    #######################################################################################\n",
    "    \"\"\" Returns elementwise sigmoid output of input array \"\"\"\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "    \n",
    "        return (1 / (1 + numpy.exp(-x)))\n",
    "        \n",
    "    #######################################################################################\n",
    "    \"\"\" Returns the convolved features of the input images \"\"\"\n",
    "    \n",
    "    def convolve(self, input_images, num_features):\n",
    "    \n",
    "        \"\"\" Extract useful values \"\"\"\n",
    "    \n",
    "        image_dim      = input_images.shape[0]\n",
    "        image_channels = input_images.shape[2]\n",
    "        num_images     = input_images.shape[3]\n",
    "        \n",
    "        \"\"\" Assign memory for the convolved features \"\"\"\n",
    "        \n",
    "        conv_dim           = image_dim - self.patch_dim + 1\n",
    "        convolved_features = numpy.zeros((num_features, num_images, conv_dim, conv_dim));\n",
    "        \n",
    "        for image_num in range(num_images):\n",
    "        \n",
    "            for feature_num in range(num_features):\n",
    "            \n",
    "                \"\"\" Initialize convolved image as array of zeros \"\"\"\n",
    "            \n",
    "                convolved_image = numpy.zeros((conv_dim, conv_dim))\n",
    "                \n",
    "                for channel in range(image_channels):\n",
    "                \n",
    "                    \"\"\" Extract feature corresponding to the indices \"\"\"\n",
    "                \n",
    "                    limit0  = self.patch_dim * self.patch_dim * channel\n",
    "                    limit1  = limit0 + self.patch_dim * self.patch_dim\n",
    "                    feature = self.W[feature_num, limit0 : limit1].reshape(self.patch_dim, self.patch_dim)\n",
    "                    \n",
    "                    \"\"\" Image to be convolved \"\"\"\n",
    "                    \n",
    "                    image = input_images[:, :, channel, image_num]\n",
    "                    \n",
    "                    \"\"\" Convolve image with the feature and add to existing matrix \"\"\"\n",
    "\n",
    "                    convolved_image = convolved_image + scipy.signal.convolve2d(image, feature, 'valid');\n",
    "                \n",
    "                \"\"\" Take sigmoid transform and store \"\"\"\n",
    "                    \n",
    "                convolved_image = self.sigmoid(convolved_image + self.b[feature_num, 0])\n",
    "                convolved_features[feature_num, image_num, :, :] = convolved_image\n",
    "                \n",
    "        return convolved_features\n",
    "        \n",
    "    #######################################################################################\n",
    "    \"\"\" Pools the given convolved features \"\"\"\n",
    "    \n",
    "    def pool(self, convolved_features):\n",
    "    \n",
    "        \"\"\" Extract useful values \"\"\"\n",
    "    \n",
    "        num_features = convolved_features.shape[0]\n",
    "        num_images   = convolved_features.shape[1]\n",
    "        conv_dim     = convolved_features.shape[2]\n",
    "        res_dim      = int(conv_dim / self.pool_dim)\n",
    "        \n",
    "        \"\"\" Initialize pooled features as array of zeros \"\"\"\n",
    "        \n",
    "        pooled_features = numpy.zeros((num_features, num_images, res_dim, res_dim))\n",
    "        \n",
    "        for image_num in range(num_images):\n",
    "        \n",
    "            for feature_num in range(num_features):\n",
    "            \n",
    "                for pool_row in range(res_dim):\n",
    "                \n",
    "                    row_start = pool_row * self.pool_dim\n",
    "                    row_end   = row_start + self.pool_dim\n",
    "                    \n",
    "                    for pool_col in range(res_dim):\n",
    "                    \n",
    "                        col_start = pool_col * self.pool_dim\n",
    "                        col_end   = col_start + self.pool_dim\n",
    "                        \n",
    "                        \"\"\" Extract image patch and calculate mean pool \"\"\"\n",
    "                        \n",
    "                        patch = convolved_features[feature_num, image_num, row_start : row_end,\n",
    "                                                   col_start : col_end]\n",
    "                        pooled_features[feature_num, image_num, pool_row, pool_col] = numpy.mean(patch)\n",
    "                        \n",
    "        return pooled_features\n",
    "        \n",
    "###########################################################################################\n",
    "\n",
    "\"\"\" The Softmax Regression class \"\"\"\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "\n",
    "    #######################################################################################\n",
    "    \"\"\" Initialization of Regressor object \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, num_classes, lamda):\n",
    "    \n",
    "        \"\"\" Initialize parameters of the Regressor object \"\"\"\n",
    "    \n",
    "        self.input_size  = input_size  # input vector size\n",
    "        self.num_classes = num_classes # number of classes\n",
    "        self.lamda       = lamda       # weight decay parameter\n",
    "        \n",
    "        \"\"\" Randomly initialize the class weights \"\"\"\n",
    "        \n",
    "        rand = numpy.random.RandomState(int(time.time()))\n",
    "        \n",
    "        self.theta = 0.005 * numpy.asarray(rand.normal(size = (num_classes*input_size, 1)))\n",
    "    \n",
    "    #######################################################################################\n",
    "    \"\"\" Returns the groundtruth matrix for a set of labels \"\"\"\n",
    "        \n",
    "    def getGroundTruth(self, labels):\n",
    "    \n",
    "        \"\"\" Prepare data needed to construct groundtruth matrix \"\"\"\n",
    "    \n",
    "        labels = numpy.array(labels).flatten()\n",
    "        data   = numpy.ones(len(labels))\n",
    "        indptr = numpy.arange(len(labels)+1)\n",
    "        \n",
    "        \"\"\" Compute the groundtruth matrix and return \"\"\"\n",
    "        \n",
    "        ground_truth = scipy.sparse.csr_matrix((data, labels, indptr))\n",
    "        ground_truth = numpy.transpose(ground_truth.todense())\n",
    "        \n",
    "        return ground_truth\n",
    "        \n",
    "    #######################################################################################\n",
    "    \"\"\" Returns the cost and gradient of 'theta' at a particular 'theta' \"\"\"\n",
    "        \n",
    "    def softmaxCost(self, theta, input, labels):\n",
    "    \n",
    "        \"\"\" Compute the groundtruth matrix \"\"\"\n",
    "    \n",
    "        ground_truth = self.getGroundTruth(labels)\n",
    "        \n",
    "        \"\"\" Reshape 'theta' for ease of computation \"\"\"\n",
    "        \n",
    "        theta = theta.reshape(self.num_classes, self.input_size)\n",
    "        \n",
    "        \"\"\" Compute the class probabilities for each example \"\"\"\n",
    "        \n",
    "        theta_x       = numpy.dot(theta, input)\n",
    "        hypothesis    = numpy.exp(theta_x)      \n",
    "        probabilities = hypothesis / numpy.sum(hypothesis, axis = 0)\n",
    "        \n",
    "        \"\"\" Compute the traditional cost term \"\"\"\n",
    "        \n",
    "        cost_examples    = numpy.multiply(ground_truth, numpy.log(probabilities))\n",
    "        traditional_cost = -(numpy.sum(cost_examples) / input.shape[1])\n",
    "        \n",
    "        \"\"\" Compute the weight decay term \"\"\"\n",
    "        \n",
    "        theta_squared = numpy.multiply(theta, theta)\n",
    "        weight_decay  = 0.5 * self.lamda * numpy.sum(theta_squared)\n",
    "        \n",
    "        \"\"\" Add both terms to get the cost \"\"\"\n",
    "        \n",
    "        cost = traditional_cost + weight_decay\n",
    "        \n",
    "        \"\"\" Compute and unroll 'theta' gradient \"\"\"\n",
    "        \n",
    "        theta_grad = -numpy.dot(ground_truth - probabilities, numpy.transpose(input))\n",
    "        theta_grad = theta_grad / input.shape[1] + self.lamda * theta\n",
    "        theta_grad = numpy.array(theta_grad)\n",
    "        theta_grad = theta_grad.flatten()\n",
    "        \n",
    "        return [cost, theta_grad]\n",
    "    \n",
    "    #######################################################################################\n",
    "    \"\"\" Returns predicted classes for a set of inputs \"\"\"\n",
    "            \n",
    "    def softmaxPredict(self, theta, input):\n",
    "    \n",
    "        \"\"\" Reshape 'theta' for ease of computation \"\"\"\n",
    "    \n",
    "        theta = theta.reshape(self.num_classes, self.input_size)\n",
    "        \n",
    "        \"\"\" Compute the class probabilities for each example \"\"\"\n",
    "        \n",
    "        theta_x       = numpy.dot(theta, input)\n",
    "        hypothesis    = numpy.exp(theta_x)      \n",
    "        probabilities = hypothesis / numpy.sum(hypothesis, axis = 0)\n",
    "        \n",
    "        \"\"\" Give the predictions based on probability values \"\"\"\n",
    "        \n",
    "        predictions = numpy.zeros((input.shape[1], 1))\n",
    "        predictions[:, 0] = numpy.argmax(probabilities, axis = 0)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "\"\"\" Loads the training images and labels \"\"\"\n",
    "    \n",
    "def loadTrainingDataset():\n",
    "\n",
    "    \"\"\" Loads the images and labels as numpy arrays\n",
    "        The dataset is originally read as a dictionary \"\"\"\n",
    "\n",
    "    train_data   = scipy.io.loadmat('stlTrainSubset.mat')\n",
    "    train_images = numpy.array(train_data['trainImages'])\n",
    "    train_labels = numpy.array(train_data['trainLabels'])\n",
    "    \n",
    "    return [train_images, train_labels]\n",
    "    \n",
    "###########################################################################################\n",
    "\"\"\" Loads the test images and labels \"\"\"\n",
    "    \n",
    "def loadTestDataset():\n",
    "\n",
    "    \"\"\" Loads the images and labels as numpy arrays\n",
    "        The dataset is originally read as a dictionary \"\"\"\n",
    "\n",
    "    test_data   = scipy.io.loadmat('stlTestSubset.mat')\n",
    "    test_images = numpy.array(test_data['testImages'])\n",
    "    test_labels = numpy.array(test_data['testLabels'])\n",
    "    \n",
    "    return [test_images, test_labels]\n",
    "\n",
    "###########################################################################################\n",
    "\"\"\" Visualizes the obtained optimal W1 values as images \"\"\"\n",
    "\n",
    "def visualizeW1(opt_W1, vis_patch_side, hid_patch_side):\n",
    "\n",
    "    \"\"\" Add the weights as a matrix of images \"\"\"\n",
    "    \n",
    "    figure, axes = matplotlib.pyplot.subplots(nrows = hid_patch_side,\n",
    "                                              ncols = hid_patch_side)\n",
    "    \n",
    "    \"\"\" Rescale the values from [-1, 1] to [0, 1] \"\"\"\n",
    "    \n",
    "    opt_W1 = (opt_W1 + 1) / 2\n",
    "    \n",
    "    \"\"\" Define useful values \"\"\"\n",
    "    \n",
    "    index  = 0\n",
    "    limit0 = 0\n",
    "    limit1 = limit0 + vis_patch_side * vis_patch_side\n",
    "    limit2 = limit1 + vis_patch_side * vis_patch_side\n",
    "    limit3 = limit2 + vis_patch_side * vis_patch_side\n",
    "                                              \n",
    "    for axis in axes.flat:\n",
    "    \n",
    "        \"\"\" Initialize image as array of zeros \"\"\"\n",
    "    \n",
    "        img = numpy.zeros((vis_patch_side, vis_patch_side, 3))\n",
    "        \n",
    "        \"\"\" Divide the rows of parameter values into image channels \"\"\"\n",
    "        \n",
    "        img[:, :, 0] = opt_W1[index, limit0 : limit1].reshape(vis_patch_side, vis_patch_side)\n",
    "        img[:, :, 1] = opt_W1[index, limit1 : limit2].reshape(vis_patch_side, vis_patch_side)\n",
    "        img[:, :, 2] = opt_W1[index, limit2 : limit3].reshape(vis_patch_side, vis_patch_side)\n",
    "        \n",
    "        \"\"\" Plot the image on the figure \"\"\"\n",
    "        \n",
    "        image = axis.imshow(img, interpolation = 'nearest')\n",
    "        axis.set_frame_on(False)\n",
    "        axis.set_axis_off()\n",
    "        index += 1\n",
    "        \n",
    "    \"\"\" Show the obtained plot \"\"\"  \n",
    "        \n",
    "    matplotlib.pyplot.show()\n",
    "    \n",
    "###########################################################################################\n",
    "\"\"\" Returns pooled features for the provided data from a trained network \"\"\"\n",
    "\n",
    "def getPooledFeatures(network, images, num_features, res_dim, step_size):\n",
    "    \n",
    "    num_images = images.shape[3]\n",
    "    \n",
    "    \"\"\" Initialize pooled features as array of zeros \"\"\"\n",
    "\n",
    "    pooled_features_data = numpy.zeros((num_features, num_images,res_dim, res_dim))\n",
    "\n",
    "    for step in range(int(num_images / step_size)):\n",
    "    \n",
    "        \"\"\" Limits to access batch of images \"\"\"\n",
    "        \n",
    "        limit0 = step_size * step\n",
    "        limit1 = step_size * (step+1)\n",
    "        \n",
    "        image_batch = images[:, :, :, limit0 : limit1]\n",
    "        \n",
    "        \"\"\" Calculate pooled features for the image batch \"\"\"\n",
    "    \n",
    "        convolved_features = network.convolve(image_batch, num_features)\n",
    "        pooled_features    = network.pool(convolved_features)\n",
    "        \n",
    "        pooled_features_data[:, limit0 : limit1, :, :] = pooled_features\n",
    "        \n",
    "        \"\"\" Avoid memory overflow \"\"\"\n",
    "        \n",
    "        del(image_batch)\n",
    "        del(convolved_features)\n",
    "        del(pooled_features)\n",
    "    \n",
    "    \"\"\" Reshape data for training / testing \"\"\"\n",
    "    \n",
    "    input_size = int(pooled_features_data.size / num_images)\n",
    "    pooled_features_data = numpy.transpose(pooled_features_data, (0, 2, 3, 1))\n",
    "    pooled_features_data = pooled_features_data.reshape(input_size, num_images)\n",
    "    \n",
    "    return pooled_features_data\n",
    " \n",
    "\n",
    "   \n",
    "###########################################################################################\n",
    "\"\"\" Loads data, trains the Convolutional Neural Network model and predicts classes for test data \"\"\"\n",
    "\n",
    "def executeConvolutionalNeuralNetwork():\n",
    "    \n",
    "    print(\"executeConvolutionalNeuralNetwork\")\n",
    "\n",
    "    \"\"\" Initialize parameters for the Convolutional Neural Network model \"\"\"\n",
    "\n",
    "    image_dim       = 64     # dimension of the input images\n",
    "    image_channels  = 3      # number of channels in the image patches\n",
    "    vis_patch_side  = 8      # side length of sampled image patches\n",
    "    hid_patch_side  = 20     # side length of representative image patches\n",
    "    pool_dim        = 19     # dimension of patches taken while pooling\n",
    "    \n",
    "    visible_size = vis_patch_side * vis_patch_side * image_channels # number of input units              192\n",
    "    hidden_size  = hid_patch_side * hid_patch_side                  # number of hidden units             400\n",
    "    res_dim      = int((image_dim - vis_patch_side + 1) / pool_dim)      # dimension of pooled features        3\n",
    "\n",
    "    \"\"\" Load parameters learned in the SparseAutoencoderLinear exercise \"\"\"\n",
    "\n",
    "    opt_param  = numpy.load('opt_param.npy')    #(154192,)\n",
    "    zca_white  = numpy.load('zca_white.npy')    #(192,192)\n",
    "    mean_patch = numpy.load('mean_patch.npy')   #(192,1)\n",
    "    \n",
    "    \"\"\" Limits to access 'W1' and 'b1' \"\"\"\n",
    "    \n",
    "    limit0 = 0\n",
    "    limit1 = hidden_size * visible_size            # 192*400=76800   \n",
    "    limit2 = 2 * hidden_size * visible_size        #153600\n",
    "    limit3 = 2 * hidden_size * visible_size + hidden_size         # 154000\n",
    "    \n",
    "    \"\"\" Extract 'W1' and 'b1' from the learned parameters \"\"\"\n",
    "    \n",
    "    opt_W1 = opt_param[limit0 : limit1].reshape(hidden_size, visible_size)   #(400*192)\n",
    "    opt_b1 = opt_param[limit2 : limit3].reshape(hidden_size, 1)              #(400 *1)\n",
    "    \n",
    "    \"\"\" Visualize the learned optimal W1 weights \"\"\"\n",
    "    \n",
    "    #visualizeW1(numpy.dot(opt_W1, zca_white), vis_patch_side, hid_patch_side)\n",
    "    \n",
    "    \"\"\" Initialize Convolutional Neural Network model \"\"\"\n",
    "     #network = ConvolutionalNeuralNetwork((400*192), (400 *1), (192,192), (192,1), 8, 19)\n",
    "    network = ConvolutionalNeuralNetwork(opt_W1, opt_b1, zca_white, mean_patch, vis_patch_side, pool_dim)\n",
    "    print(type(network))\n",
    "    \"\"\" Step size for the pooling process\n",
    "        Pooling done iteratively to avoid memory overflow \"\"\"\n",
    "    \n",
    "    step_size = 50\n",
    "    \n",
    "    \"\"\" Load training and test data\n",
    "        Labels are mapped from [1, 2, 3, 4] to [0, 1, 2, 3] \"\"\"\n",
    "    \n",
    "    \n",
    "    # from matplotlib import pyplot as plt\n",
    "    #plt.imshow(train_images[:,:,:,100], interpolation='nearest') this code to encode image 100\n",
    "    #plt.show()\n",
    "    \n",
    "    train_images, train_labels = loadTrainingDataset()\n",
    "    test_images, test_labels   = loadTestDataset()\n",
    "    train_labels = train_labels - 1\n",
    "    test_labels  = test_labels - 1\n",
    "    \n",
    "    \"\"\" Get pooled features for training and test data \"\"\"\n",
    "    \n",
    "    softmax_train_data = getPooledFeatures(network, train_images, hidden_size, res_dim, step_size)\n",
    "    softmax_test_data  = getPooledFeatures(network, test_images, hidden_size, res_dim, step_size)\n",
    "    \n",
    "    \"\"\" Initialize parameters of the Regressor \"\"\"\n",
    "    \n",
    "    input_size     = hidden_size * res_dim * res_dim  # input vector size\n",
    "    num_classes    = 4                                # number of classes\n",
    "    lamda          = 0.0001                           # weight decay parameter\n",
    "    max_iterations = 200                              # number of optimization iterations\n",
    "    \n",
    "    \"\"\" Initialize Softmax Regressor with the above parameters \"\"\"\n",
    "    \n",
    "    regressor = SoftmaxRegression(input_size, num_classes, lamda)\n",
    "    \n",
    "    \"\"\" Run the L-BFGS algorithm to get the optimal parameter values \"\"\"\n",
    "    \n",
    "    opt_solution  = scipy.optimize.minimize(regressor.softmaxCost, regressor.theta, \n",
    "                                            args = (softmax_train_data, train_labels,), method = 'L-BFGS-B', \n",
    "                                            jac = True, options = {'maxiter': max_iterations})                                        \n",
    "    opt_theta     = opt_solution.x\n",
    "    \n",
    "    \"\"\" Obtain predictions from the trained model \"\"\"\n",
    "    \n",
    "    predictions = regressor.softmaxPredict(opt_theta, softmax_test_data)\n",
    "    \n",
    "    \"\"\" Print accuracy of the trained model \"\"\"\n",
    "    \n",
    "    correct = test_labels[:, 0] == predictions[:, 0]\n",
    "    print (\"\"\"Accuracy :\"\"\", numpy.mean(correct))\n",
    "\n",
    "executeConvolutionalNeuralNetwork()\n",
    "print (\"ali\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
